{
    database {
        profile = "slick.jdbc.HsqldbProfile$"
        db {
            driver = "org.hsqldb.jdbcDriver"
            url = """
            jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
            shutdown=false;
            hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
            hsqldb.result_max_memory_rows=10000;
            hsqldb.large_data=true;
            hsqldb.applog=1;
            hsqldb.lob_compressed=true;
            hsqldb.script_format=3
            """
            connectionTimeout = 120000
            numThreads = 1
            }
    }

    concurrent-job-limit = 2
    max-concurrent-workflows = 1

    call-caching {
    # Allows re-use of existing results for jobs you've already run
    # (default: false)
    enabled = true

    # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
    # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
    # (default: true)
    invalidate-bad-cache-results = true
  }

  SLURM {
  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    config {
      runtime-attributes = """
      Int runtime_minutes = 1440
      Int cpus = 4
      Int requested_memory_mb_per_core = 8000
      String queue = "ei-medium"
      """

      submit = """
          sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \
          ${"-c " + cpus} \
          --mem-per-cpu ${requested_memory_mb_per_core} \
          --wrap "/bin/bash ${script}"
      """
      kill = "scancel ${job_id}"
      check-alive = "squeue -j ${job_id}"
      job-id-regex = "Submitted batch job (\\d+).*"
    }
  }
}
